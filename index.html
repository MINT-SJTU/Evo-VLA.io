<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding</h1>
        <div class="is-size-5 publication-authors">
          <span class="author-block"><a href="#" target="_blank"><b>Tao Lin</b></a><sup>*</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Gen Li</b></a><sup>*</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Yilei Zhong</b></a>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Yanwen Zou</b></a>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Yuxin Du</b></a>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Jiting Liu</b></a>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Encheng Gu</b></a>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Bo Zhao</b></a><sup>†</sup></span>
        </div>


                <div class="is-size-5 publication-authors">
                  <span class="author-block">
                    <sup>1</sup>School of AI, Shanghai Jiao Tong University, 
                    <sup>2</sup>EvoMind Tech, 
                    <sup>3</sup>IAAR-Shanghai,
                    <sup>4</sup>University of Cambridge
                    <br>
                  </span>
                  <span class="eql-cntrb">
                    <small><br><sup>*</sup>Equal Contribution, <sup>†</sup>Corresponding Author</small>
                  </span>
                </div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://www.arxiv.org/pdf/2507.00416" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/MINT-SJTU/Evo-VLA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://www.arxiv.org/abs/2507.00416" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Vision-Language-Action (VLA) models have emerged as a promising framework for enabling generalist robots capable of perceiving, reasoning, and acting in the real world. These models usually build upon pretrained Vision-Language Models (VLMs), which excel at semantic understanding due to large-scale image and text pretraining. However, existing VLMs typically lack precise spatial understanding capabilities, as they are primarily tuned on 2D image-text pairs without 3D supervision. 
To address this limitation, recent approaches have incorporated explicit 3D inputs such as point clouds or depth maps, but this necessitates additional depth sensors or pre-trained depth estimation models, which may yield defective results.
In contrast, our work introduces a plug-and-play module that implicitly incorporates 3D geometry features into VLA models by leveraging an off-the-shelf visual geometry foundation model.
This integration provides the model with depth-aware visual representations, improving its ability to understand the geometric structure of the scene and the spatial relationships among objects from RGB images alone. 
We evaluate our method on a set of spatially challenging tasks in both simulation and the real world. Extensive evaluations show that our method significantly improves the performance of state-of-the-art VLA models across diverse scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Single fixed image display -->
<section class="section" style="padding-bottom: 1rem;">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Architecture of Evo-0</h2>
    <img src="static/images/Evo0_model.png" alt="Result Image" style="max-width: 100%; height: auto;">
  </div>
</section>

<!-- Single fixed image display -->
<section class="section" style="padding-bottom: 1rem;">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Simulation Experiments</h2>
    <img src="static/images/rlbench.png" alt="Result Image" style="max-width: 100%; height: auto;">
  </div>
</section>

<!-- Single fixed image display -->
<section class="section" style="padding-bottom: 1rem;">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Real-world Experiments</h2>
    <img src="static/images/task_setup_3.jpg" alt="Result Image" style="max-width: 100%; height: auto;">
  </div>
</section>
<section class="section" style="padding-bottom: 1rem;">
  <div class="container is-max-desktop">
    <ol class="content has-text-justified">
      <li>
        <strong>Centering a cylinder on a target.</strong><br>
        
        The robot is required to align a cylindrical object precisely at the center of a marked target area on the table.
        This task resembles target shooting: the target has concentric rings, and scoring is based on which ring the center of the cylinder falls into.
        The closer to the center, the higher the score.
      </li>

      <li>
        <strong>Peg-in-hole insertion.</strong><br>
        This task requires the robot to insert a cylindrical peg into one of three tightly fitting holes on a board.
        This necessitates accurate alignment in 3D space, as small tilting or offset could cause task failure.
      </li>

      <li>
        <strong>Middle bottle grasping.</strong><br>
        Three bottles are closely placed in a row, and the robot is instructed to pick the middle one.
        This setup mimics a grocery store scenario, where items are densely arranged on shelves.
        Success is defined as picking up the middle bottle without knocking over the adjacent ones.
      </li>

      <li>
        <strong>Can pick-and-place.</strong><br>
        In this task, the robot must pick up a standard can and place it in a designated spot on a shelf.
        The location of the placement is varied across trials in both position and height, requiring the model to generalize spatial understanding to different configurations.
      </li>

      <li>
        <strong>Transparent object pick-and-place.</strong><br>
        The task setup is similar to the previous one, but involves transparent objects such as glass bottles.
        This presents additional challenge, since transparent materials are often poorly captured by RGB sensors and are prone to glare, making them difficult to perceive and localize.
      </li>
    </ol>
  </div>
</section>
<section class="section" style="padding-bottom: 1rem;">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Qualitative results of our model in real-world tasks</h2>
    <img src="static/images/task_results.png" alt="Result Image" style="max-width: 100%; height: auto;">
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Continuous Rollouts without Clipping (10x)</h2>

    <!-- Group 1 -->
    <div class="columns is-vcentered">
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Baseline(Pi0)</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/pi0_task1.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Ours</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/vggt_task1.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    <!-- Group 2 -->
    <div class="columns is-vcentered">
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Baseline(Pi0)</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/pi0_task2.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Ours</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/vggt_task2.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    <!-- Group 3 -->
    <div class="columns is-vcentered">
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Baseline(Pi0)</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/pi0_task3.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column has-text-centered">
        <h4 class="subtitle is-5">Ours</h4>
        <video controls style="width: 100%; max-width: 100%;">
          <source src="static/videos/vggt_task3.mp4" type="video/mp4">
        </video>
      </div>
    </div>

  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{lin2025evo,
  title={Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding},
  author={Lin, Tao and Li, Gen and Zhong, Yilei and Zou, Yanwen and Zhao, Bo},
  journal={arXiv preprint arXiv:2507.00416},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
